{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e465f04-0c61-454a-98af-3d14b696db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "#permite que le coloquemos ; en vez de show()\n",
    "import seaborn as sns \n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression , LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac39dfa9-50e8-414e-bb1f-d3637ef51696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning\n",
      "tipo de algortimo de aprendizaje por refuerzo\n",
      "le permite a un agente imaginarioaprender a tomar decisiones optimas\n",
      "definine la accion y estado y la calidad de los pasos\n"
     ]
    }
   ],
   "source": [
    "print(\"Q-learning\")\n",
    "print(\"tipo de algortimo de aprendizaje por refuerzo\")\n",
    "print(\"le permite a un agente imaginarioaprender a tomar decisiones optimas\")\n",
    "print(\"definine la accion y estado y la calidad de los pasos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30620a43-3a44-4bc3-a80f-fb3ef7229fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensiones = (5,5)\n",
    "estado_inicial = (0,0)\n",
    "estado_objetivo = (4,4)\n",
    "obstaculos = [(1,1),(1,3),(2,3),(3,0)]\n",
    "acciones =[(-1,0),(1,0),(0,-1),(0,1)]\n",
    "num_estados = dimensiones[0]*dimensiones [1]\n",
    "num_acciones =len(acciones)\n",
    "Q=np.zeros((num_estados, num_acciones))\n",
    "def estado_a_indice(estado):\n",
    "    return estado[0]*dimensiones[1]+estado[1]\n",
    "\n",
    "#alpha = factor de la tasa de aprendizaje \n",
    "#controla cuando se actualiza el factor Q eb cada \n",
    "#paso del aprendizaje \n",
    "#gamma = factor de descuento la importancia de las \n",
    "#recompensas a futuro \n",
    "#epsilon = el agente no repita las mismas decisiones \n",
    "#episodios = nos dice el nro de veces que se repite \n",
    "alpha = 0.1 \n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "episodios = 100\n",
    "\n",
    "def elegir_accion (estado):\n",
    "    if random.uniform(0,1)< epsilon:\n",
    "        return random.choice(range(num_acciones))\n",
    "    else:\n",
    "        return np.argmax(Q[estado_a_indice(estado)])\n",
    "\n",
    "def aplicar_accion(estado, accion_idx):\n",
    "    accion =acciones[accion_idx]\n",
    "    nuevo_estado = tuple(np.add(estado,accion)%dimensiones)\n",
    "    if nuevo_estado in obstaculos or nuevo_estado == estado:\n",
    "        return estado, -100, False\n",
    "    if nuevo_estado == estado_objetivo:\n",
    "        return nuevo_estado, 100, True\n",
    "    return nuevo_estado, -1, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97b46247-2cbe-482f-bb1f-7f8bb36c8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodio in range(episodios):\n",
    "    estado = estado_inicial\n",
    "    terminado =False\n",
    "    while not terminado:\n",
    "        idx_estado = estado_a_indice(estado)\n",
    "        accion_idx=elegir_accion(estado)\n",
    "        nuevo_estado,recompensa, terminado = aplicar_accion(estado,accion_idx)\n",
    "        idx_nuevo_estado = estado_a_indice(nuevo_estado)\n",
    "        Q[idx_estado, accion_idx]= Q[idx_estado,accion_idx]+alpha+(recompensa+gamma*np.max(Q[idx_nuevo_estado])-Q[idx_estado,accion_idx])\n",
    "        estado = nuevo_estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "471909b5-12a9-4563-b491-2e3e1b816f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politica apredida: (0 es arriba, 1 es abajo, 2 es izquierda, 3 es derecha\n",
      "[[0 0 3 3 0]\n",
      " [0 0 0 0 2]\n",
      " [0 1 0 0 0]\n",
      " [0 0 2 3 1]\n",
      " [2 2 3 2 0]]\n"
     ]
    }
   ],
   "source": [
    "politica = np.zeros(dimensiones,dtype=int)\n",
    "for i in range(dimensiones[0]):\n",
    "    for j in range(dimensiones[1]):\n",
    "        estado = (i,j)\n",
    "        idx_estado = estado_a_indice(estado)\n",
    "        mejor_accion = np.argmax(Q[idx_estado])\n",
    "        politica[i,j]=mejor_accion\n",
    "print(\"politica apredida: (0 es arriba, 1 es abajo, 2 es izquierda, 3 es derecha\")\n",
    "print(politica)\n",
    "#nos da una matriz donde cada nunero es la mejor posicion a tomar \n",
    "#en esa ubicacion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236dcdda-cafc-4a1a-b983-e92ae619cb36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
